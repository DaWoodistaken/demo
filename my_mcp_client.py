import asyncio
import ollama
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# --- AYARLAR ---
MCP_SERVER_SCRIPT = "my_demo_server.py"  
MODEL_NAME = "llama3.2"                

async def run_chat_loop():
    # 1. Sunucu Parametrelerini Ayarla
    server_params = StdioServerParameters(
        command="python", 
        args=[MCP_SERVER_SCRIPT], 
        env=None
    )

    print(f"ğŸ”Œ MCP Sunucusuna BaÄŸlanÄ±lÄ±yor ({MCP_SERVER_SCRIPT})...")

    # 2. Sunucuya BaÄŸlan (Stdio Ã¼zerinden)
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            
            # 3. Sunucudaki Tool'larÄ± Listele
            await session.initialize()
            mcp_tools = await session.list_tools()
            
            # 4. MCP Tool formatÄ±nÄ± Ollama formatÄ±na Ã§evir
            # (Ollama JSON formatÄ± bekler, MCP'den geleni dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz)
            ollama_tools = []
            for tool in mcp_tools.tools:
                ollama_tools.append({
                    'type': 'function',
                    'function': {
                        'name': tool.name,
                        'description': tool.description,
                        'parameters': tool.inputSchema
                    }
                })
            
            print(f"ğŸ› ï¸  YÃ¼klenen Tool SayÄ±sÄ±: {len(ollama_tools)}")
            print("-" * 50)
            print("ğŸ¤– Asistan HazÄ±r! (Ã‡Ä±kmak iÃ§in 'q' bas)")

            # --- SOHBET DÃ–NGÃœSÃœ ---
            history = [] # Sohbet geÃ§miÅŸi
            
            while True:
                user_input = input("\nSen: ")
                if user_input.lower() in ['q', 'exit']:
                    break
                
                # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle
                history.append({'role': 'user', 'content': user_input})

                # 5. Ollama'ya GÃ¶nder (Tool'larla birlikte!)
                response = ollama.chat(
                    model=MODEL_NAME,
                    messages=history,
                    tools=ollama_tools, # âœ¨ SÄ°HÄ°R BURADA: ToollarÄ± modele veriyoruz
                )

                # 6. Model Tool Ã‡aÄŸÄ±rmak Ä°stiyor mu Kontrol Et
                if response['message'].get('tool_calls'):
                    print("\nâš¡ MODEL TOOL Ã‡AÄIRMAYA KARAR VERDÄ°:")
                    
                    # Tool Ã§aÄŸrÄ±sÄ±nÄ± geÃ§miÅŸe ekle (Modelin kafasÄ± karÄ±ÅŸmasÄ±n)
                    history.append(response['message'])

                    for tool_call in response['message']['tool_calls']:
                        tool_name = tool_call['function']['name']
                        tool_args = tool_call['function']['arguments']
                        
                        print(f"   â” Ã‡aÄŸrÄ±lan Tool: {tool_name}")
                        print(f"   â” ArgÃ¼manlar: {tool_args}")

                        # 7. Tool'u MCP Sunucusunda Ã‡alÄ±ÅŸtÄ±r
                        result = await session.call_tool(tool_name, tool_args)
                        
                        print(f"   â” MCP Sonucu: {result.content[0].text}")

                        # 8. Sonucu Ollama'ya Geri GÃ¶nder
                        history.append({
                            'role': 'tool',
                            'content': result.content[0].text,
                        })

                    # Tool sonuÃ§larÄ±yla birlikte modele tekrar sor (Final cevap iÃ§in)
                    final_response = ollama.chat(model=MODEL_NAME, messages=history)
                    print(f"\nğŸ¤– Asistan: {final_response['message']['content']}")
                    history.append(final_response['message'])

                else:
                    # Tool Ã§aÄŸÄ±rmadÄ±ysa normal cevap ver
                    print(f"ğŸ¤– Asistan: {response['message']['content']}")
                    history.append(response['message'])

if __name__ == "__main__":
    asyncio.run(run_chat_loop())